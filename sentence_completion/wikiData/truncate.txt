For other uses, see Truncation (disambiguation). In mathematics and computer science, truncation is limiting the number of digits right of the decimal point, by discarding the least significant ones. For example, consider the real numbers  5.6341432543653654 32.438191288 −6.3444444444444  To truncate these numbers to 4 decimal digits, we only consider the 4 digits to the right of the decimal point. The result would be:  5.6341 32.4381 −6.3444  Truncation is equivalent to rounding towards zero (or rounding down the absolute value of the number while maintaining the sign). The truncation error can be twice the maximum error in rounding.[citation needed]    Contents   1 Truncation and floor function 2 Causes of truncation 3 In algebra 4 See also 5 References 6 External links    Truncation and floor function[edit] Main article: Floor and ceiling functions Truncation of positive real numbers can be done using the floor function. Given a number  to be truncated and , the number of elements to be kept behind the decimal point, the truncated value of x is    However, for negative numbers truncation does not round in the same direction as the floor function: truncation always rounds toward zero, the floor function rounds towards negative infinity. Causes of truncation[edit] With computers, truncation can occur when a decimal number is typecast as an integer; it is truncated to zero decimal digits because integers cannot store real numbers (that are not themselves integers). In algebra[edit] An analogue of truncation can be applied to polynomials. In this case, the truncation of a polynomial P to degree n can be defined as the sum of all terms of P of degree n or less. Polynomial truncations arise in the study of Taylor polynomials, for example.[1] See also[edit]  Arithmetic precision Floor function Quantization (signal processing) Precision (computer science) Truncation (statistics)  References[edit]   ^ Spivak, Michael (2008). Calculus (4th ed.). p. 434. ISBN 978-0-914098-91-1.    External links[edit]  Wall paper applet that visualizes errors due to finite precision     